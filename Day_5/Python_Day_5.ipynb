{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Python_Day_5.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1J_VwH0BZl_NnrWBHnK-cScNcjzQWZXEG","authorship_tag":"ABX9TyNBWA3BEmQcEWTG1OCqoh9I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EWB457PFT4J2"},"source":["# **Introduction to Python. Day 5**\n","\n","## *Dr Kirils Makarovs*\n","\n","## *k.makarovs@exeter.ac.uk*\n","\n","## *University of Exeter Q-Step Centre*\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"6b_C8WzNqV4z"},"source":["# **Welcome to Day 5!**"]},{"cell_type":"markdown","metadata":{"id":"ZSbdhnpgS34g"},"source":["## **By now, you should be familiar with:**\n","\n","+ The overall workflow of Jupyter Notebooks in Google Colab\n","+ The basics of Python syntax and operations with lists\n","+ How to read in external datasets\n","+ How to navigate datasets - subsetting, accessing rows/columns\n","+ Operations with variables i.e. recoding, creating new variables\n","+ Exploratory data analysis\n","+ Data visualization\n"]},{"cell_type":"markdown","metadata":{"id":"DmD95g7Na2wJ"},"source":["## **Today, you are going to work on:**\n","\n","+ The final data analysis report\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hlyQaFb3PFbh"},"source":["# **1. Preparing to work in Python**"]},{"cell_type":"code","metadata":{"id":"Gi-tODR_pzmZ"},"source":["# Import the necessary libraries\n","\n","import pandas as pd # data analysis and management library\n","import numpy as np # multi-dimensional arrays\n","import math # library with math-related commands like square root, etc.\n","import random # random number generator via random.sample()\n","\n","# Data visualization libraries\n","\n","import seaborn as sns # easy-syntax plots\n","import matplotlib.pyplot as plt # deep-level library used to tweak the details of the seaborn plots\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5XOVOH_1qrIB"},"source":["# Mount your Google Drive\n","\n","# Mounting your Google Drive will enable you to access files from Drive in Google Colab e.g. datasets, notebooks, etc.\n","\n","from google.colab import drive\n","\n","# This will prompt for authorization. Enter your authorisation code and rerun the cell\n","\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZ57vKoXWdZZ"},"source":["## **Python Data Science Handbook by Jake VanderPlas**\n","\n","<figure>\n","<left>\n","<img src=https://jakevdp.github.io/PythonDataScienceHandbook/figures/PDSH-cover.png  width=\"500\">\n","</figure>\n","\n","*Book is available [here](https://jakevdp.github.io/PythonDataScienceHandbook/).*\n","\n","*Reproducible notebooks for the handbook are available [here](https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks).*"]},{"cell_type":"markdown","metadata":{"id":"2L4TkrbyWw1w"},"source":["---\n","\n","# **2. Exporting Colab notebooks**\n","\n","+ *Rough-and-ready PDF:* `File -> Print`\n","\n","(doesn't always work though and you may lose text if it goes beyond the page limits)\n","\n","+ *Exporting as HTML:* Instructions with screenshots available [here](https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab).\n","\n","    + **Step #1:** Save a copy of the notebook in `.ipynb` format onto your machine and give it a different name. For this, use `File -> Download .ipynb`\n","    + **Step #2:** Upload the saved notebook into the current Colab session by going to `Files` and clicking on `Upload to session storage`\n","    + **Step #3:** The notebook should now be visible in the root folder of `Files`. Click on three dots and get the path to the notebook\n","    + **Step #4:** In the current notebook, create a new code cell and paste the following piece of code:\n","\n","```\n","%%shell\n","jupyter nbconvert --to html <notebook_directory.ipynb>\n","```\n","+ \n","  + **Step #5:** Change `<notebook_directory.ipynb>` to the pathway that you got in **Step #3** and run the code cell\n","  + **Step #6:** The HTML file should now be visible in the root folder of `Files`. Click on three dots and download it onto your machine\n","  + **Step #7:** Find the HTML file on your machine and click on it. It should open in browser\n","  + **Step #8 (additional):** You can save HTML file as PDF by clicking `ctrl/command + P` in your browser\n"]},{"cell_type":"code","metadata":{"id":"PpLSRtzP7oru"},"source":["%%shell\n","jupyter nbconvert --to html /content/HTML_Python_Day_5.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yf2BT7PTOdAy"},"source":["---\n","\n","# **3. Data analysis report**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Dlq6Js848lSy"},"source":["You are going to work with the **Data Science Job Salaries Dataset**\n","\n","This is a small dataset on data science job market collected in 2020 and 2021 in various countries. It containts information on 245 employees.\n","\n","The dataset has the following variables:\n","\n","+ **work_year**: The year during which the salary was paid\n","+ **experience_level**: The experience level in the job\n","+ **employment_type**: The type of employement for the role\n","+ **job_title**: The role worked in during the year\n","+ **salary**: The total gross salary amount paid\n","+ **salary_currency**: The currency of the salary\n","+ **salary_in_usd**: The salary in US dollars\n","+ **employee_residence**: Employee's primary country of residence in during the work\n","+ **remote_ratio**: The overall amount of work done remotely\n","+ **company_location**: The country of the employer's main office or contracting branch\n","+ **company_size**: The average number of people that worked for the company during the year"]},{"cell_type":"code","metadata":{"id":"lnBPCsn8LjWn"},"source":["# Let's get the dataset first!\n","\n","# Note that your pathway to the file might be different from mine\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Intro_to_python/Day_5/Data_Science_Jobs_Salaries.csv')\n","\n","# Data source: https://www.kaggle.com/saurabhshahane/data-science-jobs-salaries\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0cYwm_0cvFbX"},"source":["df.shape # 245 observation and 11 variables\n","\n","df.columns # names of the variables\n","\n","df.info() # there are 2 numeric variables (salary, salary_in_usd). All other variables should be treated as categorical\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJ5ojXY0vvjR"},"source":["# (just run this cell before starting your data analysis)\n","\n","# Let me clean the dataset a bit for you:\n","\n","##########\n","\n","# work_year variable now has two clean values: 2020 and 2021\n","df['work_year'].replace(['2021e'], ['2021'], inplace=True)\n","\n","##########\n","\n","# experience_level variable now has interpretable levels\n","df['experience_level'].replace(['MI', 'SE', 'EN', 'EX'],\n","                        ['Intermediate', 'Expert', 'Junior', 'Director'], inplace=True)\n","\n","##########\n","\n","# employment_type variable now has interpretable levels\n","df['employment_type'].replace(['FT', 'PT', 'CT', 'FL'],\n","                        ['Full-time', 'Part-time', 'Contract', 'Freelance'], inplace=True)\n","\n","# Additionally, because of low frequencies, I combine Part-time, Contract, and Freelance into one group\n","df['employment_type'] = np.where(df['employment_type'].isin(['Part-time', 'Contract', 'Freelance']), 'Part-Time/Contract/Freelance', df['employment_type'])\n","\n","##########\n","\n","# There were quite a lot of job titles in the job_title variable who had low frequency (less than 5), so I combine them all into\n","# one category and give it a name of 'Other'\n","\n","# Creating a list job titles for whom frequency is less than 5\n","jobs_low_fr = df['job_title'].value_counts()[df['job_title'].value_counts() < 5].index\n","\n","# Overriding the job_title variable by saying that everyone with infrequent jobs will go into 'Other' category,\n","# and the rest will keep their original job titles\n","df['job_title'] = np.where(df['job_title'].isin(jobs_low_fr), 'Other', df['job_title'])\n","\n","##########\n","\n","# There were quite a lot of countries in the employee_residence variable who had low frequency (less than 7), so I combine them all into\n","# one category and give it a name of 'Other'\n","\n","# Creating a list of countries for whom frequency is less than 7\n","countries_res_low_fr = df['employee_residence'].value_counts()[df['employee_residence'].value_counts() < 7].index\n","\n","# Overriding the employee_residence variable by saying that everyone with infrequent countries will go into 'Other' category,\n","# and the rest will keep their original countries of residence\n","df['employee_residence'] = np.where(df['employee_residence'].isin(countries_res_low_fr), 'Other', df['employee_residence'])\n","\n","##########\n","\n","# remote_ratio variable now has interpretable levels\n","df['remote_ratio'].replace([0, 50, 100],\n","                        ['No remote work', 'Partially remote', 'Fully remote'], inplace=True)\n","\n","##########\n","\n","# There were quite a lot of countries in the company_location variable who had low frequency (less than 7), so I combine them all into\n","# one category and give it a name of 'Other'\n","\n","# Creating a list of countries for whom frequency is less than 7\n","countries_loc_low_fr = df['company_location'].value_counts()[df['company_location'].value_counts() < 7].index\n","\n","# Overriding the company_location variable by saying that everyone with infrequent countries will go into 'Other' category,\n","# and the rest will keep their original countries of residence\n","df['company_location'] = np.where(df['company_location'].isin(countries_loc_low_fr), 'Other', df['company_location'])\n","\n","##########\n","\n","# company_size variable now has interpretable levels\n","df['company_size'].replace(['L', 'S', 'M'],\n","                        ['Large', 'Small', 'Medium'], inplace=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OgWdpp4n-eoz"},"source":["# Get the head of the cleaned dataset\n","\n","df.head(10)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hGV7ACciOtdp"},"source":["---\n","\n","# **4. Your task**\n","\n","*For the final data analysis report, please:*\n","\n","1. form groups of 2-3 people\n","2. come up with a small research question that can be answered with the help of the provided dataset (we will discuss some of the examples in class)\n","3. produce 2-4 tables **and** 2-4 graphs that address your research question \n","4. interpret the tables and graphs that you have obtained (you don't have to write any text)\n","5. send your graphs and tables to k.makarovs@exeter.ac.uk either as a **single** Notebook, **single** PDF file, or **single** HTML file\n","**(single HTML file is preferable)**\n","6. present your findings to the class and let's discuss them!\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hUNIKwt5PGrG"},"source":["*Here is a cheetsheet with handy Python commands that you can use for the final data analysis report:*\n","\n","| Command | Description |\n","| ------ | ----------- |\n","| `df['variable'].value_counts()`   | Frequency table |\n","| `pd.crosstab(df['variable_1'], df['variable_2']`) | Crosstab |\n","| `df.groupby('variable_1')['variable_2'].mean()` | Aggregated analysis|\n","| `sns.histplot()`, `sns.countplot()`, `sns.scatterplot()`, etc. | Seaborn graphs|\n","| `np.where(condition, value if True, value if False)` | Creating new variable|\n","\n","Don't forget that you can also subset the dataset via `.loc[]` method if you want to focus on particular rows/columns of the dataset!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QTwRcWrEQx1B"},"source":["# **That's the end of Day 5!**"]}]}